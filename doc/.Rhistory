sentence.list%>%
filter(File=="JohnFKennedy",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(25)
sentence.list%>%
filter(File=="JohnFKennedy",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="JohnFKennedy",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(11)
sentence.list%>%
filter(File=="JohnFKennedy",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="DwightDEisenhower",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(20)
sentence.list%>%
filter(File=="DonaldJTrump",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(25)
sentence.list%>%
filter(File=="DonaldJTrump",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="BarackObama",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="DonaldJTrump",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="RonaldReagan",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="WilliamJClinton",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="RonaldReagan",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="RonaldReagan",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="RonaldReagan",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
?sample_n
set.seed(123)
sentence.list%>%
filter(File=="DonaldJTrump",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="BarackObama",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="RonaldReagan",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
set.seed(123)
sentence.list%>%
filter(File=="DonaldJTrump",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="WilliamJClinton",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="AbrahamLincoln",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
set.seed(123)
sentence.list%>%
filter(File=="DonaldJTrump",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="WilliamJClinton",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="RichardNixon",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
sentence.list%>%
filter(File=="AbrahamLincoln",
Term == 1,
word.count >=3, word.count<=10)%>%
select(sentences)%>%sample_n(10)
heatmap.2(cor(sentence.list%>%filter(Term ==1)%>%select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
par(mar=c(4, 6, 2, 1))
emo.means=colMeans(select(sentence.list, anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1",
"chartreuse3", "blueviolet",
"darkgoldenrod2", "dodgerblue3",
"darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
heatmap.2(cor(sentence.list%>%filter(Term ==1)%>%select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
par(mar=c(4, 6, 2, 1))
emo.means=colMeans(select(sentence.list, anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1",
"chartreuse3", "blueviolet",
"darkgoldenrod2", "dodgerblue3",
"darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
emo.means=colMeans(sentence.list%>%filter(Party == Democratic)%>%select(anger:trust)>0.01)
emo.means=colMeans(sentence.list%>%filter(Party == "Democratic")%>%select(anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1",
"chartreuse3", "blueviolet",
"darkgoldenrod2", "dodgerblue3",
"darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
col.use=c("red2", "darkgoldenrod1",
"chartreuse3", "blueviolet",
"darkgoldenrod2", "dodgerblue3",
"darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
```
emo.means.dem=colMeans(sentence.list%>%filter(Party == "Democratic")%>%select(anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1",
"chartreuse3", "blueviolet",
"darkgoldenrod2", "dodgerblue3",
"darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means.dem[order(emo.means.dem)], las=2, col=col.use[order(emo.means.dem)], horiz=T, main="Inaugural Speeches")
par(mar=c(4, 6, 2, 1), mfrow = c(2,1))
# emo.means=colMeans(select(sentence.list, anger:trust)>0.01)
emo.means.dem=colMeans(sentence.list%>%filter(Party == "Democratic")%>%select(anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1",
"chartreuse3", "blueviolet",
"darkgoldenrod2", "dodgerblue3",
"darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means.dem[order(emo.means.dem)], las=2, col=col.use[order(emo.means.dem)], horiz=T, main="Inaugural Speeches")
emo.means.rep=colMeans(sentence.list%>%filter(Party == "Republican")%>%select(anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1",
"chartreuse3", "blueviolet",
"darkgoldenrod2", "dodgerblue3",
"darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means.rep[order(emo.means.rep)], las=2, col=col.use[order(emo.means.rep)], horiz=T, main="Inaugural Speeches")
corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
View(corpus.list)
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]
docs <- Corpus(VectorSource(corpus.list$snipets))
?Corpus
head(docs)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
sample(1:5,1)
writeLines(as.character(docs[1]))
docs <-tm_map(docs,content_transformer(tolower))
writeLines(as.character(docs[1]))
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[1]))
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[1]))
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[1]))
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[1]))
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[1]))
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- paste(corpus.list$File,
corpus.list$Term, corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]
View(corpus.list)
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- paste(corpus.list$File,
corpus.list$Term, corpus.list$Party, corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]
View(corpus.list)
head(dtm)
dtm[1]
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 15
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
ldaOut.terms <- as.matrix(terms(ldaOut,20))
View(ldaOut.topics)
topicProbabilities <- as.data.frame(ldaOut@gamma)
View(topicProbabilities)
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
presid.summary=tbl_df(sentence.list)%>%
filter(Term == 1)%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
5)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
presid.summary=tbl_df(sentence.list)%>%
filter(Term == 1)%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
2)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
km.res
km.res$cluster
test <- speech%>%
group_by(File)%>%
select(File, Party)%>%
order(File)
test <- speech%>%
group_by(File)%>%
select(File, Party)
View(test)
test <- speech%>%
distinct(File)%>%
select(File, Party)
test <- speech%>%
distinct(File)
View(test)
test <- speech%>%
select(distinct(File), Party)
test <- speech%>%
select(distinct(File), Party)
test <- speech%>%
distinct(File,Party)%>%
select(File, Party)
View(test)
test <- speech%>%
distinct(File,Party)%>%
select(File, Party)%>%
arrange(File)
test2 <- km.res$cluster
test
test2
View(speech)
test <- test[-14, ]
test <- cbind(test, test2)
View(test)
test <- test%>%
filter(Party = "Republican" | Party = "Democratic")
test <- test%>%
filter(Party == "Republican" | Party == "Democratic")
test <- speech%>%
distinct(File,Party, Date)%>%
select(File, Party, Date)%>%
arrange(File)
test <- test[-14, ]
test <- test%>%
filter(Party == "Republican" | Party == "Democratic")
test <- speech%>%
distinct(File,Party, Date)%>%
select(File, Party, Date)%>%
arrange(File)
test <- cbind(test, test2)
View(test)
test <- speech%>%
distinct(File, .keep_all = TRUE)%>%
select(File, Party, Date)%>%
arrange(File)
View(test)
test <- test[-14, ]
test <- cbind(test, test2)
test <- test%>%
filter(Party == "Republican" | Party == "Democratic")
test <- test%>%
arrange(date)
test <- test%>%
arrange(Date)
presid.summary=tbl_df(sentence.list)%>%
filter(Term == 1, Date > 1900-01-01)%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
2)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
View(presid.summary)
presid.summary=tbl_df(sentence.list)%>%
filter(Term == 1, Date > 1829-01-01)%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
2)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
presid.summary=tbl_df(sentence.list)%>%
filter(Term == 1, Date > 1829-01-01)%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
View(sentence.list)
View(sentence.list)
sentence.list$Date[3679] > 1900-01-01
sentence.list$Date[3679] < 1900-01-01
sentence.list$Date[3679]
sentence.list$Date[4964]
sentence.list$Date[4964] > 1900-01-01
1941-01-20 >1900-01-01
presid.summary=tbl_df(sentence.list)%>%
filter(Term == 1)%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
4)
test2 <- km.res$cluster
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
test <- speech%>%
distinct(File, .keep_all = TRUE)%>%
select(File, Party, Date)%>%
arrange(File)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
presid.summary=tbl_df(sentence.list)%>%
filter(Term == 1, Party == "Republican"  |Party == "Democratic")%>%
#group_by(paste0(type, File))%>%
group_by(File)%>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust)
#negative=mean(negative),
#positive=mean(positive)
)
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200,
4)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
fviz_cluster(km.res,
stand=F, repel= TRUE,
data = presid.summary[,-1], xlab="", xaxt="n",
show.clust.cent=FALSE)
topics.hash=c("Economy", "America", "Defense", "Belief", "Election", "Patriotism", "Unity", "Government", "Reform", "Temporal", "WorkingFamilies", "Freedom", "Equality", "Misc", "Legislation")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]
colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
class(sentence.list$Date[3679])
z <- Sys.Date()
z+10
z < c("2009-06-01", "2010-01-01", "2015-01-01")
sentence.list$Date[3679] > as.date(1900-01-01)
sentence.list$Date[3679] > as.Date(1900-01-01)
as.Date(1900-01-01)
as.Date(1900/01/01)
sentence.list$Date[3679] > "1900-01-01"
sentence.list$Date[4964] > "1900-01-01"
