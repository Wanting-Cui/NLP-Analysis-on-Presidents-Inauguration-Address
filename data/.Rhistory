dists <- rowSums((Lags - rep(NewPoint, each = n))^2)
neighbors <- order(dists)[1:K]
neighb.dir <- Dir[neighbors]
choice <- names(which.max(table(neighb.dir)))
return(choice)
}
KNNclass(c(-.5, .5, -.5, -.5, .5))
KNNclass <- function(NewPoint, K = 5, Lags = Weekly[, 2:6],
Dir = Weekly$Direction) {
n <- nrow(LagData)
stopifnot(length(NewPoint) == 5, ncol(Lags) == 5, K <= n)
dists <- rowSums((Lags - rep(NewPoint, each = n))^2)
neighbors <- order(dists)[1:K]
neighb.dir <- Dir[neighbors]
choice <- names(which.max(table(neighb.dir)))
return(choice)
}
KNNclass(c(-.5, .5, -.5, -.5, .5))
KNNclass <- function(NewPoint, K = 5, Lags = Weekly[, 2:6],
Dir = Weekly$Direction) {
n <- nrow(Lags)
stopifnot(length(NewPoint) == 5, ncol(Lags) == 5, K <= n)
dists <- rowSums((Lags - rep(NewPoint, each = n))^2)
neighbors <- order(dists)[1:K]
neighb.dir <- Dir[neighbors]
choice <- names(which.max(table(neighb.dir)))
return(choice)
}
KNNclass(c(-.5, .5, -.5, -.5, .5))
test.error <- mean(predictions != test$Direction)
test <- Weekly[Weekly$Year >= 2009, ]
train <- Weekly[Weekly$Year < 2009, ]
n.test <- nrow(test)
prediction <- rep(NA, n.test)
for (i in 1:n.test){
newp <- as.numeric(test[i, 2:6])
predictions[i] <- KNNclass(newp, Lags =train[, 2:6],
Dir = train$Direction)
}
test.error <- mean(predictions != test$Direction)
predictions <- rep(NA, n.test)
for (i in 1:n.test){
newp <- as.numeric(test[i, 2:6])
predictions[i] <- KNNclass(newp, Lags =train[, 2:6],
Dir = train$Direction)
}
test.error <- mean(predictions != test$Direction)
test.error
k <- 9
nums <- rep(1:k, each = nrow(Weekly)/k)
fold <- sample(nums)
View(mpg)
ggplot(data = mpg)+
geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
ggplot(data = mpg)+
geom_point(mapping = aes(x = dsipl, y = hwy, color = class))
ggplot(data = mpg)+
geom_point(mapping = aes(x = displ, y = hwy, color = class))
data <- readLines("BaltimoreHomicides.txt")
khloe <- grep("Khloe Lewis", data)
khloe
deaths1 <- grep("shooting", data)
deaths2 <- grep("Shooting", data)
length(deaths1)
length(deaths2)
setdiff(deaths1, deaths2)
data[105]
r       <- regexpr(">Age:\\s.*year(s)? old<", data)
age_vec <- regmatches(data, r)
head(age_vec)
age_vec <- substring(age_vec, 2, nchar(age_vec) - 1)
head(age_vec)
library(MASS)
quantile(cats$Hwt, c(0.25, 0.5, 0.75))
seq(1:20, length.out = 5)
seq(1,20, length.out = 5)
n = 100
sds <- rep(seq(1,20, length.out = 5), each = n)
head(sds)
samp <- matrix(c(sds, NA), ncol = 2, byrow = FALSE)
samp <- matrix(c(sds, rep(NA, n*5)), ncol = 2, byrow = FALSE)
head(samp)
sds <- rep(c(5,10,25,50,100)), each = n)
sds <- rep(c(5,10,25,50,100), each = n)
samp <- matrix(c(size, rep(NA, n*5)), ncol = 2, byrow = FALSE)
size <- rep(c(5,10,25,50,100), each = n)
samp <- matrix(c(size, rep(NA, n*5)), ncol = 2, byrow = FALSE)
cal.mean <- function(num){
return(mean(rnorm(num), 10, 3))
}
samp[,2] <- sapply(samp[,1], cal.mean)
head(samp)
library(ggplot2)
colnames(samp) <- c("size", "mean")
facet_wrap(~size)
head(samp)
facet_wrap(~size)
ggplot(data.frame(samp))+
geom_histogram(aes(x = mean, y = ..desnity..))+
geom_density(aes(x = mean))+
facet_wrap(~size)
ggplot(data.frame(samp))+
geom_histogram(aes(x = mean, y = ..density..))+
geom_density(aes(x = mean))+
facet_wrap(~size)
library("alr4")
lm1 <- lm(lifeExpF ~ group, data = UN11)
lm1 <- lm(lifeExpF ~ group, data = UN11)
lm2 <- lm(lifeExpF ~ 1+group, data = UN11)
summary(lm1)
summary(lm2)
lm3 <- lm(LifeExpF ~ group*log(ppgdp), data = UN11)
lm3 <- lm(lifeExpF ~ group*log(ppgdp), data = UN11)
summary(lm3)$coefficients
library(alr4)
library(alr4)
class(UN11$region); levels(UN11$region)
class(UN11$group); levels(UN11$group)
head(model.matrix(~-1+group,UN11),10)
Boxplot(lifeExpF~group,data=UN11,id.n=2)
lsmeans(m0,pairwise~group)
library(lsmeans)
install.packages("lsmeans")
library(lsmeans)
install.packages("lsmeans")
library(lsmeans)
lsmeans(m0,pairwise~group)
plot(Effect("group",lm(lifeExpF~group,data=UN11)),main='')
scatterplot(lifeExpF~log(ppgdp)|group,data=UN11,smooth=FALSE,ylab="female life expectancy")
m1<-lm(lifeExpF~group+log(ppgdp),UN11); summary(m1)
plot(allEffects(m1,default.levels=50),ylim=c(60,85),grid=TRUE,multiline=TRUE)
m2<-lm(lifeExpF~group*log(ppgdp),UN11); summary(m2)
plot(Effect(c("group","ppgdp"),m2,default.levels=100),rug=FALSE,grid=TRUE,multiline=TRUE)
plot(Effect(c("group","ppgdp"),m2,default.levels=100),xlab="log(ppgdp)",rug=FALSE,grid=TRUE,multiline=TRUE,transform.x=list(ppgdp=list(trans=log,inverse=exp)),ticks.x=list(at=c(100,1000,5000,30000)))
fuel2001<-transform(fuel2001,Dlic=1000*Drivers/Pop,Fuel=1000*FuelC/Pop,Income=Income/1000)
pairs(~Fuel+Tax+Dlic+Income+log(Miles),data=fuel2001)
m0<-lm(Fuel~1,fuel2001) # null model with no regressors beyond the intercept
m1<-update(m0,~Tax) # use only Tax
m2<-update(m1,~.+Dlic) # use Tax and Dlic
m3<-update(m2,~.+Income) # use Tax, Dlic, and Income
m4<-update(m3,~.+log(Miles)) # use 4 regressors beyond the intercept
anova(m0,m4)
anova(m4) # Type I ANOVA; Sequential F-tests: use RMS from m4 to estimate error variance
Anova(m4) # Type II ANOVA: intercept isn't listed in table
Anova(m4,type=3); summary(m4)
anova(m4) # Type I ANOVA; Sequential F-tests: use RMS from m4 to estimate error variance
u1<-lm(lifeExpF~group,UN11)
u2<-lm(lifeExpF~log(ppgdp),UN11)
u3<-lm(lifeExpF~group+log(ppgdp),UN11)
u4<-lm(lifeExpF~group*log(ppgdp),UN11)
anova(u3,u4)
anova(u2,u3)
anova(u2,u3,u4) # use RMS from u4 to estimate error variance
anova(u3,u4)
anova(u3,u4)
anova(u2,u3)
anova(u2,u3,u4) # use RMS from u4 to estimate error variance
coef(u4)
linearHypothesis(u4,L)
L<-matrix(c(0,1,-1,0,0,0,0,0,0,0,1,-1),byrow=TRUE,nrow=2)
linearHypothesis(u4,L)
coef(u4)
summary(u4)
library(alr4)
names(Rateprof)
pairs(~quality+easiness+raterInterest,data=Rateprof)
r1<-lm(quality~easiness+raterInterest,weights=numRaters,data=Rateprof)
summary(r1)
residuals(r1)
residuals(r1,type="pearson")
names(sniffer)
pairs(~Y+TankTemp+GasTemp+TankPres+GasPres,data=sniffer)
s1<-lm(Y~.,sniffer); summary(s1)$coef
vcov(s1)
summary(r1)
pairs(~Y+TankTemp+GasTemp+TankPres+GasPres,data=sniffer)
s1<-lm(Y~.,sniffer); summary(s1)$coef
vcov(s1)
sqrt(diag(vcov(s1)))
hccm(s1,type="hc3") # Heteroscedasticity-Corrected Covariance Matrices
library(lmtest)
coeftest(s1,vcov=hccm)
ncvTest(s1) # 'ncvTest' is a function in Package "car"
ncvTest(s1,~TankTemp+GasTemp)
coeftest(s1,vcov=hccm)
ncvTest(s1) # 'ncvTest' is a function in Package "car"
ncvTest(s1,~TankTemp+GasTemp)
coeftest(s1,vcov=hccm)
hccm(s1,type="hc3") # Heteroscedasticity-Corrected Covariance Matrices
ncvTest(s1) # 'ncvTest' is a function in Package "car"
ncvTest(s1,~TankTemp+GasTemp)
data(Stevens)
print(xyplot(y~loudness,group=subject,data=Stevens,type=c("l","g","p"),xlab="Loudness (dB)",ylab="Average log(length)"))
library(nlme)
print(xyplot(y~loudness,group=subject,data=Stevens,type=c("l","g","p"),xlab="Loudness (dB)",ylab="Average log(length)"))
ncvTest(s1) # 'ncvTest' is a function in Package "car"
ncvTest(s1,~TankTemp+GasTemp)
m.boot<-Boot(m.ols,R=999,method="case")
m.ols<-lm(time~t1+t2,data=Transact)
m.ols<-lm(time~t1+t2,data=Transact)
5rsummary(m.ols)
summary(m.ols)
m.boot<-Boot(m.ols,R=999,method="case")
summary(m.boot)
confint(m.ols,level=.95) # Normal theory
ans <- confint(m.ols,level=.95)[1,2]-summary(m.boot)[1,2]
ans/2
ans/2+12.426
confint(m.boot,level=.95,type="bca") # BCa method
confint(m.boot,level=.95,type="norm") # Normal with boot SE
confint(m.ols,level=.95) # Normal theory
pairs(~time+t1+t2,data=Transact) # data is included in the package 'car'
m.ols<-lm(time~t1+t2,data=Transact)
summary(m.ols)
(out1<-deltaMethod(m.ols,"t1/t2"))
deltaMethod(m.ols,"((Intercept)+t1)/((Intercept)+t2)")
m.boot<-Boot(m.ols,R=999,method="case")
summary(m.boot)
confint(m.ols,level=.95) # Normal theory
confint(m.boot,level=.95,type="bca") # BCa method
m.boot<-Boot(m.ols,R=999,method="case")
summary(m.boot)
head(brains)
plot(BrainWt~BodyWt,brains)
head(ufcwc)
with(ufcwc,invTranPlot(Dbh,Height))
unlist(with(ufcwc,invTranEstimate(Dbh,Height))) # get optimal lambda without drawing graph
head(Highway)
pairs(~rate+len+adt+trks+slim+shld+sigs,data=Highway)
Highway$sigs1<-with(Highway,(sigs*len+1)/len)
summary(powerTransform(cbind(len,adt,trks,shld,sigs1)~1,Highway))
pairs(~rate+log(len)+log(adt)+log(trks)+slim+shld+log(sigs1),data=Highway)
m2<-lm(rate~log(len)+log(adt)+log(trks)+slim+shld+log(sigs1),Highway)
inverseResponsePlot(m2)
inverseResponsePlot(m2)
boxCox(m2)
summary(powerTransform(m2))
summary(boxCox(m2))
bcPower(m2)
summary(powerTransform(m2))
names(caution)
c1<-lm(y~x1+x2,caution)
residualPlot(c1,quadratic=FALSE) # Pearson residuals versus fitted values
fuel2001<-transform(fuel2001,Dlic=1000*Drivers/Pop,Fuel=1000*FuelC/Pop,Income=Income/1000)
pairs(~Fuel+Tax+Dlic+Income+log(Miles),data=fuel2001)
f1<-lm(Fuel~Tax+Dlic+Income+log(Miles),fuel2001)
hatvalues(f1)
residualPlots(f1,id.method=c("AK","WY","DC")) # array of plots of Pearson residuals versus each regressor & fitted values
names(UN11)
pairs(~fertility+log(ppgdp)+lifeExpF,data=UN11)
u1<-lm(fertility~log(ppgdp)+lifeExpF,UN11)
residuals(u1)[1:5]
residuals(u1,type="pearson")[1:5] # Pearson residuals; same as ordinary residuals because W = I
rstandard(u1)[1:5] # standardized residuals
rstudent(u1)[1:5] # Studentized residuals
outlierTest(u1)
u1<-lm(fertility~log(ppgdp)+lifeExpF,UN11)
residuals(u1)[1:5]
rstandard(u1)[1:5] # standardized residuals
rstudent(u1)[1:5] # Studentized residuals
rstudent(u1) # Studentized residuals
residuals(u1)[1:5]
rstudent(u1)[1:5] # Studentized residuals
cooks.distance(u1)[1:5]
influence(u1,do.coef=TRUE)$coefficients # returns a matrix whose i^{th} row is \hat{\beta}_{(i)}.
influenceIndexPlot(u1) # plots Cook's distance etc. versus case number
cooks.distance(u1)[1:5]
influence(u1,do.coef=TRUE)$coefficients # returns a matrix whose i^{th} row is \hat{\beta}_{(i)}.
plot(dheight~mheight,Heights)
h1<-lm(dheight~mheight,data=Heights); summary(h1)
qqnorm(residuals(h1)); qqline(residuals(h1))
shapiro.test(residuals(h1)) # Shapiro-Wilk test of normality
pairs(~time+t1+t2,data=Transact) # data is included in the package 'car'
m1<-lm(time~t1+t2,data=Transact); summary(m1)
qqnorm(residuals(m1)); qqline(residuals(m1))
shapiro.test(residuals(m1))
names(MinnWater)
anova(lm(log(muniUse)~muniPrecip+log(muniPop)+year,data=MinnWater))
pairs(~log(muniUse)+year+muniPrecip+log(muniPop),data=MinnWater)
m1<-lm(log(muniUse)~year,data=MinnWater); summary(m1)
m3<-lm(log(muniUse)~year+muniPrecip+log(muniPop),data=MinnWater); summary(m3)
vif(m3)
anova(lm(log(muniUse)~muniPrecip+log(muniPop)+year,data=MinnWater))
setwd("~/GitHub/Spring2018-Project1-Wanting-Cui/data")
library(xlsx)
library(reshape2)
library(dplyr)
dates <- read.table("InauguationDates.txt", header = TRUE, fill = TRUE, sep = "\t")
list.files()
setwd("~/GitHub/Spring2018-Project1-Wanting-Cui/data")
library(xlsx)
library(reshape2)
library(dplyr)
setwd("~/GitHub/Spring2018-Project1-Wanting-Cui/data")
dates <- read.table("InauguationDates.txt", header = TRUE, fill = TRUE, sep = "\t")
list.files()
read.table("InauguationDates.txt", header = TRUE, fill = TRUE, sep = "\t")
dates <- read.table("InauguationDates.txt", header = TRUE, fill = TRUE, sep = "\t")
info <- read.xlsx("InaugurationInfo.xlsx", 1,header = TRUE)
info <- read.xlsx("InaugurationInfo.xlsx", 1,header = TRUE)
?root.dir
getwd()
dates <- read.table("../data/InauguationDates.txt", header = TRUE, fill = TRUE, sep = "\t")
info <- read.xlsx("../data/InaugurationInfo.xlsx", 1,header = TRUE)
dates <- read.table("../data/InauguationDates.txt", header = TRUE, fill = TRUE, sep = "\t")
info <- read.xlsx("../data/InaugurationInfo.xlsx", 1,header = TRUE)
dates <- read.table("../data/InauguationDates.txt", header = TRUE, fill = TRUE, sep = "\t")
info <- read.xlsx("../data/InaugurationInfo.xlsx", 1,header = TRUE)
dates2 <- melt(dates, id = "PRESIDENT")
dates2$variable <- as.numeric(as.factor(dates2$variable))
del <- which(dates2$value == "")
dates2 <- dates2[-del, ]
# Manually change some presidents' names, to make names identical in two dataframes
dates2$PRESIDENT <- as.character(dates2$PRESIDENT)
info$President <- as.character(info$President)
dates2[8,1] <- toString("Martin van Buren")
dates2[11,1] <- toString("James K. Polk")
dates2[20,1] <- toString("James Garfield")
info[25,1] <- toString("Grover Cleveland")
info[27,1] <- toString("Grover Cleveland")
dates2[which(dates2$PRESIDENT == "Richard M. Nixon"),1] <- "Richard Nixon"
# Merge df based on president and term
speech <- info %>%
left_join(dates2, by = c("President" = "PRESIDENT")) %>%
filter(Term == variable)
speech<- speech[,-6]
colnames(speech)[6] <- "Date"
# Manually delete an error in df test
speech <- speech[-47,]
View(info)
View(dates2)
dates <- read.table("../data/InauguationDates.txt", header = TRUE, fill = TRUE, sep = "\t")
info <- read.xlsx("../data/InaugurationInfo.xlsx", 1,header = TRUE)
dates2 <- melt(dates, id = "PRESIDENT")
dates2$variable <- as.numeric(as.factor(dates2$variable))
del <- which(dates2$value == "")
dates2 <- dates2[-del, ]
dates2$PRESIDENT <- as.character(dates2$PRESIDENT)
info$President <- as.character(info$President)
dates2[8,1] <- toString("Martin van Buren")
dates2[11,1] <- toString("James K. Polk")
dates2[20,1] <- toString("James Garfield")
info[25,1] <- toString("Grover Cleveland")
info[27,1] <- toString("Grover Cleveland")
dates2[which(dates2$PRESIDENT == "Richard M. Nixon"),1] <- "Richard Nixon"
speech <- info %>%
left_join(dates2, by = c("President" = "PRESIDENT")) %>%
filter(Term == variable)
View(speech)
speech<- speech[,-"variable"]
speech<- speech[,-which(colnames(speech) =="variable")]
colnames(speech)[6] <- "Date"
speech$Fulltext <- NULL
View(speech)
speech$Fulltext <- NA
combtext <- function(df){
exp <- paste("../data/InauguralSpeeches/inaug","df$File","-","df$Term",".txt" sep = "")
exp <- paste("../data/InauguralSpeeches/inaug","df$File","-","df$Term","//.txt" sep = "")
exp <- paste("../data/InauguralSpeeches/inaug","df$File","-","df$Term","//.txt",sep = "")
combtext <- function(df){
exp <- paste("../data/InauguralSpeeches/inaug","df$File","-","df$Term","//.txt",sep = "")
df$Fulltext <- scan(exp)
}
paste("../data/InauguralSpeeches/inaug","speech$File","-","speech$Term","//.txt",sep = "")
paste("../data/InauguralSpeeches/inaug",speech$File,"-",speech$Term,"//.txt",sep = "")
paste("../data/InauguralSpeeches/inaug",speech$File,"-",speech$Term,".txt",sep = "")
combtext <- function(df){
exp <- paste("../data/InauguralSpeeches/inaug",df$File,"-",df$Term,".txt",sep = "")
df$Fulltext <- scan(exp)
}
test <- apply(speech, 1, combtext)
combtext <- function(df){
exp <- paste("../data/InauguralSpeeches/inaug",df[2],"-",df[3],".txt",sep = "")
df$Fulltext <- scan(exp)
}
test <- apply(speech, 1, combtext)
df$Fulltext <- readLines(exp. collapse = " ")
df$Fulltext <- readLines(exp, collapse = " ")
?readLines
exp <- paste("../data/InauguralSpeeches/inaug",speech[1,2],"-",speech[1,3],".txt",sep = "")
exp
scan(exp, what = "")
read.table(exp)
paste(readLines(exp),collapse=" ")
paste(readLines(exp),collapse=" ")
combtext <- function(df){
exp <- paste("../data/InauguralSpeeches/inaug",df[2],"-",df[3],".txt",sep = "")
df$Fulltext <- paste(readLines(exp), collapse = " ")
}
test <- apply(speech, 1, combtext)
View(speech)
speech <- speech[-41, ]
test <- apply(speech, 1, combtext)
View(speech)
readLines(exp)
combtext <- function(df){
exp <- paste("../data/InauguralSpeeches/inaug",df[2],"-",df[3],".txt",sep = "")
df[7] <- paste(readLines(exp), collapse = " ")
}
test <- apply(speech, 1, combtext)
View(speech)
dim(test)
speech.test <- speech
combtext <- function(df){
exp <- paste("../data/InauguralSpeeches/inaug",df[2],"-",df[3],".txt",sep = "")
return(paste(readLines(exp), collapse = " "))
}
speech.test$Fulltext <- apply(speech.test,1,combtext)
View(speech.test)
speech$Fulltext <- apply(speech,1,combtext)
View(speech)
View(info)
sentence.list=NULL
for(i in 1:nrow(speech)){
sentences=sent_detect(speech$fulltext[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(speech[i,-ncol(speech)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
library("rvest")
library("tibble")
library("qdap")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RANN")
library("topicmodels")
source("C:/Users/Wanting/Documents/GitHub/ADS_Teaching/Tutorials/wk2-TextMining/lib/plotstacked.R")
source("C:/Users/Wanting/Documents/GitHub/ADS_Teaching/Tutorials/wk2-TextMining/lib/speechFuncs.R")
sentence.list=NULL
for(i in 1:nrow(speech)){
sentences=sent_detect(speech$fulltext[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(speech[i,-ncol(speech)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
sentence.list=NULL
for(i in 1:nrow(speech)){
sentences=sent_detect(speech$Fulltext[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(speech[i,-ncol(speech)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
sentence.list=NULL
for(i in 1:nrow(speech)){
sentences=sent_detect(speech$Fulltext[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(speech[i,-ncol(speech)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
View(sentence.list)
